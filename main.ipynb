{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO2HpSxG/S30W+fJ6SYcCnZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrancescoI/RecoUtils/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6dKyGRMXZgM",
        "outputId": "6f7d17cb-c1de-4b12-cac9-a099ebf9061f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iRB3DYyPa5-"
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "import scipy.sparse as sp\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMMduOxXfgxx"
      },
      "source": [
        "def gpu(tensor, gpu=False):\n",
        "\n",
        "    if gpu:\n",
        "        return tensor.cuda()\n",
        "    else:\n",
        "        return tensor\n",
        "\n",
        "\n",
        "def cpu(tensor):\n",
        "\n",
        "    if tensor.is_cuda:\n",
        "        return tensor.cpu()\n",
        "    else:\n",
        "        return tensor"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWCLjrSlQcJv"
      },
      "source": [
        "### Loss"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wx5YWDe-QdLe"
      },
      "source": [
        "def hinge_loss(positive, negative):\n",
        "    \n",
        "    loss = torch.clamp(negative - positive + 1.0, 0.0)\n",
        "\n",
        "    return loss.mean()"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZqTTsToP8VS"
      },
      "source": [
        "### Utilities"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pq4g1zHhP9nH"
      },
      "source": [
        "def get_negative_batch(batch, n_items, item_to_metadata_dict):\n",
        "    \n",
        "    neg_batch = None\n",
        "\n",
        "    neg_item_id = np.random.randint(0, n_items-1, len(batch))\n",
        "    \n",
        "    if item_to_metadata_dict:\n",
        "        neg_metadata_id = [item_to_metadata_dict[item] for item in neg_item_id]    \n",
        "    else:\n",
        "        neg_metadata_id = None\n",
        "    \n",
        "    neg_batch = pd.concat([neg_batch, pd.DataFrame({'user_id': batch['user_id'],\n",
        "                                                    'item_id': neg_item_id, \n",
        "                                                    'metadata': neg_metadata_id})])\n",
        "            \n",
        "    return neg_batch"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mhWqe6JQP72"
      },
      "source": [
        "class Dataset():\n",
        "    \n",
        "    def __init__(self, brand):\n",
        "        \n",
        "        self.brand = brand\n",
        "    \n",
        "    def _get_interactions(self):\n",
        "\n",
        "        bucket_uri = f'/content/drive/My Drive/'\n",
        "\n",
        "        if self.brand == 'missoni':\n",
        "        \n",
        "          dataset = pd.read_csv(bucket_uri + f'{self.brand}.csv')\n",
        "\n",
        "\n",
        "        elif self.brand == 'ton':\n",
        "          \n",
        "          clickstream = pd.read_csv(bucket_uri + f'{self.brand}.csv')\n",
        "          metadata = pd.read_csv(bucket_uri + 'anagrafica_ton.csv')\n",
        "\n",
        "          clickstream = (clickstream\\\n",
        "                         .groupby(['user_ids', 'product_code'])['brand'].count() \n",
        "                         ).reset_index()\n",
        "          \n",
        "          clickstream.columns = ['hashedEmail', 'product', 'actions']\n",
        "\n",
        "          dataset = pd.merge(clickstream, metadata, left_on='product', right_on='pty_pim_variant')\n",
        "          \n",
        "          dataset = dataset[['hashedEmail', 'product', 'macro', 'saleline', 'actions']]\n",
        "          dataset.columns = ['hashedEmail', 'product', 'macro', 'saleLine', 'actions']\n",
        "\n",
        "          dataset['gender'] = 'W'\n",
        "\n",
        "        return dataset \n",
        "    \n",
        "    \n",
        "    def _encondig_label(self, dataset, input_col, output_col):\n",
        "        \n",
        "        encoder = LabelEncoder()\n",
        "        dataset[output_col] = encoder.fit(dataset[input_col]).transform(dataset[input_col])\n",
        "        \n",
        "        return dataset, encoder\n",
        "    \n",
        "    \n",
        "    def fit(self, metadata=None, seasons=None):\n",
        "        \n",
        "        dataset = self._get_interactions()\n",
        "        self.metadata = metadata\n",
        "        \n",
        "        if seasons:\n",
        "            dataset = dataset[dataset['season'].isin(seasons)]\n",
        "        \n",
        "        ### Label Encoding\n",
        "        dataset, _ = self._encondig_label(dataset, input_col='hashedEmail', output_col='user_id')\n",
        "        dataset, _ = self._encondig_label(dataset, input_col='product', output_col='item_id')\n",
        "        \n",
        "        if metadata is not None:\n",
        "            output_list_name = []\n",
        "            \n",
        "            for meta in metadata:\n",
        "                output_name = meta + '_id'\n",
        "                dataset, _ = self._encondig_label(dataset, input_col=meta, output_col=output_name)\n",
        "                output_list_name.append(output_name)                \n",
        "            \n",
        "            dataset['metadata'] = dataset[output_list_name].values.tolist()\n",
        "            self.metadata_id = output_list_name\n",
        "            \n",
        "        self.dataset = dataset\n",
        "        \n",
        "    def get_item_metadata_dict(self):\n",
        "        \n",
        "        if self.metadata is not None:\n",
        "        \n",
        "            return self.dataset.set_index('item_id')['metadata'].to_dict()\n",
        "        \n",
        "        else:\n",
        "            \n",
        "            return None"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wv7ONQu8P3-l"
      },
      "source": [
        "### Evaluate"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN5JU_vlP5UI"
      },
      "source": [
        "def auc_score(positive, negative):\n",
        "    \n",
        "    total_auc = []\n",
        "    \n",
        "    positive = positive.cpu().detach().numpy()\n",
        "    negative = negative.cpu().detach().numpy()\n",
        "\n",
        "    batch_auc = (positive > negative).sum() / len(positive)\n",
        "    total_auc.append(batch_auc)\n",
        "        \n",
        "    return np.mean(total_auc)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Em05GKUsQAT8"
      },
      "source": [
        "def evaluate(model):\n",
        "    \n",
        "    ### TRAIN    \n",
        "    negative_train = get_negative_batch(batch=model.train, n_items=model.dataset.dataset['item_id'].max(), item_to_metadata_dict=None)\n",
        "    negative_train.columns = ['user_id', 'item_id_neg', 'metadata_neg']\n",
        "    \n",
        "    merged_train = pd.concat([model.train, negative_train], axis=1)\n",
        "    \n",
        "    train_auc = []\n",
        "    \n",
        "    for row in merged_train.itertuples():\n",
        "        \n",
        "        score = np.sum(model.pred[row.user_id, row.item_id] > model.pred[row.user_id, row.item_id_neg])\n",
        "        train_auc.append(score)\n",
        "    \n",
        "    ### TEST\n",
        "    negative_test = get_negative_batch(batch=model.test, n_items=model.dataset.dataset['item_id'].max(), item_to_metadata_dict=None)\n",
        "    negative_test.columns = ['user_id', 'item_id_neg', 'metadata_neg']\n",
        "    merged_test = pd.concat([model.test, negative_test], axis=1)\n",
        "    \n",
        "    test_auc = []\n",
        "    \n",
        "    for row in merged_test.itertuples():\n",
        "        \n",
        "        score = np.sum(model.pred[row.user_id, row.item_id] > model.pred[row.user_id, row.item_id_neg])\n",
        "        test_auc.append(score)\n",
        "    \n",
        "    print(f'Train AUC: {np.sum(train_auc) / merged_train.shape[0]}')\n",
        "    print(f'Test AUC: {np.sum(test_auc) / merged_test.shape[0]}')"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocus4Q3oQCmL"
      },
      "source": [
        "def precision_recall_k(model, k):\n",
        "    \n",
        "    ### TRAIN\n",
        "    values = np.ones(model.train.shape[0])\n",
        "    users = model.train.loc[:, 'user_id']\n",
        "    items = model.train.loc[:, 'item_id']\n",
        "    \n",
        "    sparse_matrix = sp.csr_matrix((values, (users, items)))\n",
        "    \n",
        "    matrix = sparse_matrix.toarray()\n",
        "    total_precision = []\n",
        "    total_recall = []\n",
        "    \n",
        "    for index, row in enumerate(matrix):\n",
        "        \n",
        "        truth = np.nonzero(row)[0]\n",
        "        if len(truth) > 0:\n",
        "            prediction = np.argsort(-model.pred[index, :])[:k]\n",
        "\n",
        "            n_matching = len(set(truth) & set(prediction))\n",
        "            precision = n_matching / k\n",
        "            recall = n_matching / len(truth)\n",
        "\n",
        "            total_precision.append(precision)\n",
        "            total_recall.append(recall)\n",
        "        \n",
        "    print(f'Train Precision@{k}: {np.mean(total_precision)} \\nTrain Recall@{k}: {np.mean(total_recall)} \\nTrain Shape: {sparse_matrix.getnnz()}')\n",
        "        \n",
        "    \n",
        "    ### TEST\n",
        "    values = np.ones(model.test.shape[0])\n",
        "    users = model.test.loc[:, 'user_id']\n",
        "    items = model.test.loc[:, 'item_id']\n",
        "    \n",
        "    sparse_matrix = sp.csr_matrix((values, (users, items)))\n",
        "    \n",
        "    matrix = sparse_matrix.toarray()\n",
        "    total_precision = []\n",
        "    total_recall = []\n",
        "    \n",
        "    for index, row in enumerate(matrix):\n",
        "        \n",
        "        truth = np.nonzero(row)[0]\n",
        "        if len(truth) > 0:\n",
        "            prediction = np.argsort(-model.pred[index, :])[:k]\n",
        "\n",
        "            n_matching = len(set(truth) & set(prediction))\n",
        "            precision = n_matching / k\n",
        "            recall = n_matching / len(truth)\n",
        "\n",
        "            total_precision.append(precision)\n",
        "            total_recall.append(recall)\n",
        "        \n",
        "    print(f'\\nTest Precision@{k}: {np.mean(total_precision)} \\nTest Recall@{k}: {np.mean(total_recall)} \\nTest Shape: {sparse_matrix.getnnz()}')"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaAZhILlPmP4"
      },
      "source": [
        "### Model"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DFmHnUNPfdH"
      },
      "source": [
        "class ScaledEmbedding(torch.nn.Embedding):\n",
        "    \"\"\"\n",
        "    Embedding layer that initialises its values\n",
        "    to using a normal variable scaled by the inverse\n",
        "    of the embedding dimension.\n",
        "    \"\"\"\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        \"\"\"\n",
        "        Initialize parameters.\n",
        "        \"\"\"\n",
        "\n",
        "        self.weight.data.normal_(0, 1.0 / self.embedding_dim)\n",
        "        if self.padding_idx is not None:\n",
        "            self.weight.data[self.padding_idx].fill_(0)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mfsFpKRPijh"
      },
      "source": [
        "class ZeroEmbedding(torch.nn.Embedding):\n",
        "    \"\"\"\n",
        "    Embedding layer that initialises its values\n",
        "    to using a normal variable scaled by the inverse\n",
        "    of the embedding dimension.\n",
        "    Used for biases.\n",
        "    \"\"\"\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        \"\"\"\n",
        "        Initialize parameters.\n",
        "        \"\"\"\n",
        "\n",
        "        self.weight.data.zero_()\n",
        "        if self.padding_idx is not None:\n",
        "            self.weight.data[self.padding_idx].fill_(0)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggjFhDEuPlGA"
      },
      "source": [
        "class CFTemplate(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self, dataset, n_factors, use_cuda=False):\n",
        "        super().__init__()\n",
        "             \n",
        "        self.dataset = dataset\n",
        "        self.n_users = dataset.dataset['user_id'].max() + 1\n",
        "        self.n_items = dataset.dataset['item_id'].max() + 1\n",
        "        self.dictionary = dataset.get_item_metadata_dict()\n",
        "        \n",
        "        self.n_factors = n_factors\n",
        "        \n",
        "        self.use_cuda = use_cuda\n",
        "\n",
        "        self.user = gpu(ScaledEmbedding(self.n_users, self.n_factors), self.use_cuda)\n",
        "        self.item = gpu(ScaledEmbedding(self.n_items, self.n_factors), self.use_cuda)\n",
        "        \n",
        "        self.user_bias = gpu(ZeroEmbedding(self.n_users, 1), self.use_cuda)\n",
        "        self.item_bias = gpu(ZeroEmbedding(self.n_items, 1), self.use_cuda)\n",
        "        \n",
        "       \n",
        "    def forward(self, net, batch, batch_size=1):\n",
        "        \n",
        "        neg_batch = get_negative_batch(batch, self.n_items, self.dictionary)\n",
        "        \n",
        "        positive = gpu(self.net(batch, batch_size), self.use_cuda)\n",
        "        negative = gpu(self.net(neg_batch, batch_size), self.use_cuda)\n",
        "        \n",
        "        return positive, negative\n",
        "    \n",
        "    \n",
        "    def backward(self, positive, negative, optimizer):\n",
        "                \n",
        "        optimizer.zero_grad()\n",
        "                \n",
        "        loss_value = hinge_loss(positive, negative)                \n",
        "\n",
        "        loss_value.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        return loss_value.item()\n",
        "    \n",
        "    \n",
        "    def fit(self, optimizer, batch_size=1024, epochs=10, split_train_test=False, verbose=False):\n",
        "        \n",
        "        if split_train_test:\n",
        "            \n",
        "            print('|== Splitting Train/Test ==|')\n",
        "            \n",
        "            data = self.dataset.dataset.iloc[np.random.permutation(len(self.dataset.dataset))]\n",
        "            train = data.iloc[:int(len(data) * 0.9)]\n",
        "            test = data.iloc[int(len(data) * 0.9):]\n",
        "            \n",
        "            print(f'Shape Train: {len(train)} \\nShape Test: {len(test)}')\n",
        "            \n",
        "        else:\n",
        "            \n",
        "            train = self.dataset.dataset\n",
        "        \n",
        "        \n",
        "        self.total_train_auc = []\n",
        "        self.total_test_auc = []\n",
        "        self.total_loss = []\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            print(f'Epoch: {epoch+1}')\n",
        "            \n",
        "            epoch_loss = []\n",
        "\n",
        "            for first in range(0, len(train), batch_size):\n",
        "                \n",
        "                batch = train.iloc[first:first+batch_size, :]     \n",
        "                                \n",
        "                net = self.net(batch, len(batch))\n",
        "                \n",
        "                positive, negative = self.forward(net, batch, len(batch))\n",
        "                \n",
        "                loss_value = self.backward(positive, negative, optimizer)\n",
        "                \n",
        "            epoch_loss.append(loss_value)\n",
        "            self.total_loss.append(epoch_loss)\n",
        "            \n",
        "            if verbose:\n",
        "                ### AUC Calc.\n",
        "\n",
        "                train_sample = train.sample(n=20_000)\n",
        "\n",
        "                net_train = self.net(train_sample, len(train_sample))\n",
        "                positive_train, negative_train = self.forward(net_train, train_sample, len(train_sample))           \n",
        "                train_auc = auc_score(positive_train, negative_train)\n",
        "                self.total_train_auc.append(train_auc)\n",
        "\n",
        "                test_sample = test.sample(n=20_000) \n",
        "                \n",
        "                net_test = self.net(test_sample, len(test_sample))\n",
        "                positive_test, negative_test = self.forward(net_test, test_sample, len(test_sample))\n",
        "                test_auc = auc_score(positive_test, negative_test)\n",
        "                self.total_test_auc.append(test_auc)\n",
        "                \n",
        "                print(f'Epoch {epoch+1}, \\n== Loss: {sum(epoch_loss)} \\n== Train AUC: {train_auc} \\n== Test AUC: {test_auc}')\n",
        "            \n",
        "    def history(self):\n",
        "        \n",
        "        return {'train_loss': self.total_loss,\n",
        "                'train_auc': self.total_train_auc,\n",
        "                'test_auc': self.total_test_auc}\n",
        "    \n",
        "    def get_item_representation(self):\n",
        "        \n",
        "        return self.item.cpu().detach().numpy()"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLVlJDL-PsCS"
      },
      "source": [
        "class LightFM(CFTemplate):\n",
        "    \n",
        "    def __init__(self, dataset, n_factors, use_metadata=True, use_cuda=False):\n",
        "        super().__init__(dataset, n_factors, use_cuda)\n",
        "        \n",
        "        self.use_metadata = use_metadata\n",
        "        self.use_cuda = use_cuda\n",
        "        \n",
        "        if use_metadata:\n",
        "            self.n_metadata = self._get_n_metadata(dataset)\n",
        "            self.metadata = gpu(ScaledEmbedding(self.n_metadata, n_factors), self.use_cuda)\n",
        "    \n",
        "    \n",
        "    def _get_n_metadata(self, dataset):\n",
        "        \n",
        "        n_metadata = 0\n",
        "        \n",
        "        for col in dataset.metadata_id:\n",
        "            n_metadata += dataset.dataset[col].max() + 1\n",
        "        \n",
        "        return n_metadata\n",
        "    \n",
        "    \n",
        "    def net(self, batch, batch_size):\n",
        "        \n",
        "        \"\"\"\n",
        "        Forward method that express the model as the dot product of user and item embeddings, plus the biases. \n",
        "        Item Embeddings itself is the sum of the embeddings of the item ID and its metadata\n",
        "        \"\"\"\n",
        "        \n",
        "        user = Variable(gpu(torch.LongTensor(batch['user_id'].values), self.use_cuda))\n",
        "        item = Variable(gpu(torch.LongTensor(batch['item_id'].values), self.use_cuda))\n",
        "        \n",
        "        if self.use_metadata:\n",
        "            metadata = Variable(gpu(torch.LongTensor(list(batch['metadata'])), self.use_cuda))\n",
        "            metadata = self.metadata(metadata)\n",
        "\n",
        "        user_bias = self.user_bias(user)\n",
        "        item_bias = self.item_bias(item)\n",
        "        \n",
        "        user = self.user(user)\n",
        "        item = self.item(item)\n",
        "        \n",
        "        if self.use_metadata:\n",
        "        \n",
        "            ### Reshaping in order to match metadata tensor\n",
        "            item = item.reshape(batch_size, 1, self.n_factors)        \n",
        "            item_metadata = torch.cat([item, metadata], axis=1)\n",
        "\n",
        "            ### sum of latent dimensions\n",
        "            item = item_metadata.sum(1)\n",
        "        \n",
        "        net = (user * item).sum(1).view(-1,1) + user_bias + item_bias\n",
        "        \n",
        "        return net\n",
        "    \n",
        "    def get_item_representation(self):\n",
        "        \n",
        "        if self.use_metadata:\n",
        "            \n",
        "            data = (self.dataset\n",
        "                    .dataset[['item_id'] + self.dataset.metadata_id]\n",
        "                    .drop_duplicates())\n",
        "            \n",
        "            mapping = pd.get_dummies(data, columns=[*self.dataset.metadata_id]).values[:, 1:]\n",
        "            identity = np.identity(self.dataset.dataset['item_id'].max() + 1)\n",
        "            binary = np.hstack([identity, mapping])\n",
        "            \n",
        "            metadata_representation = np.vstack([self.item.weight.detach().numpy(), self.metadata.weight.detach().numpy()])\n",
        "            \n",
        "            return np.dot(binary, metadata_representation), binary, metadata_representation\n",
        "        \n",
        "        else:\n",
        "            return self.item.weight.cpu().detach().numpy()\n",
        "        \n",
        "        \n",
        "    def predict(self, user_idx):\n",
        "        \n",
        "        \"\"\"\n",
        "        It takes a user vector representation (based on user_idx arg) and it takes the dot product with\n",
        "        the item representation\n",
        "        \"\"\"\n",
        "        \n",
        "        item_repr, _, _ = self.get_item_representation()\n",
        "        user_repr = self.user.weight.detach().numpy()\n",
        "        \n",
        "        item_bias = self.item_bias.weight.detach().numpy()\n",
        "        user_bias = self.user_bias[torch.tensor([user_idx])].detach().numpy()\n",
        "        \n",
        "        return np.dot(user_pred[user_idx, :], item_repr) + item_bias + user_bias"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCVIuFwIPun0"
      },
      "source": [
        "class MLP(CFTemplate):\n",
        "    \n",
        "    def __init__(self, dataset, n_factors, use_metadata=True, use_cuda=False):\n",
        "        super().__init__(dataset, n_factors, use_cuda)\n",
        "        \n",
        "        self.use_metadata = use_metadata\n",
        "        self.use_cuda = use_cuda\n",
        "\n",
        "        if use_metadata:\n",
        "            self.n_metadata = self._get_n_metadata(dataset)\n",
        "            self.n_metadata_type = self._get_n_metadata_type(dataset)\n",
        "            self.metadata = gpu(ScaledEmbedding(self.n_metadata, n_factors), self.use_cuda)\n",
        "            \n",
        "        else:\n",
        "            self.n_metadata_type = 0\n",
        "        \n",
        "        self.linear_1 = gpu(torch.nn.Linear(n_factors*(2+self.n_metadata_type), int(self.n_factors/2)), self.use_cuda)\n",
        "        self.linear_2 = gpu(torch.nn.Linear(int(self.n_factors/2), int(self.n_factors/4)), self.use_cuda)\n",
        "        self.linear_3 = gpu(torch.nn.Linear(int(self.n_factors/4), 1), self.use_cuda)\n",
        "        \n",
        "    def _get_n_metadata(self, dataset):\n",
        "        \n",
        "        n_metadata = 0\n",
        "        \n",
        "        for col in dataset.metadata_id:\n",
        "            n_metadata += dataset.dataset[col].max() + 1\n",
        "        \n",
        "        return n_metadata\n",
        "    \n",
        "    def _get_n_metadata_type(self, dataset):\n",
        "        \n",
        "        return len(dataset.metadata)\n",
        "    \n",
        "    \n",
        "    def mlp(self, dataset, batch_size=1):\n",
        "        \n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        user = gpu(torch.from_numpy(dataset['user_id'].values), self.use_cuda)\n",
        "        item = gpu(torch.from_numpy(dataset['item_id'].values), self.use_cuda)\n",
        "        \n",
        "        if self.use_metadata:\n",
        "            metadata = Variable(gpu(torch.LongTensor(list(dataset['metadata'])), self.use_cuda))\n",
        "            metadata = self.metadata(metadata).reshape(batch_size, self.n_factors*self.n_metadata_type)\n",
        "            \n",
        "        user = self.user(user)\n",
        "        item = self.item(item)\n",
        "        \n",
        "        if self.use_metadata:\n",
        "            cat = torch.cat([user, item, metadata], axis=1).reshape(batch_size, (2+self.n_metadata_type)*self.n_factors)\n",
        "        else:\n",
        "            cat = torch.cat([user, item], axis=1).reshape(batch_size, 2*self.n_factors)\n",
        "                \n",
        "        net = self.linear_1(cat)\n",
        "        net = torch.nn.functional.relu(net)\n",
        "        \n",
        "        net = self.linear_2(net)\n",
        "        net = torch.nn.functional.relu(net)\n",
        "        \n",
        "        net = self.linear_3(net)\n",
        "        \n",
        "        return net\n",
        "    \n",
        "    def net(self, dataset, batch_size=1):\n",
        "        \n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        \n",
        "        net = gpu(self.mlp(dataset, batch_size), self.use_cuda)\n",
        "                \n",
        "        return net\n",
        "    \n",
        "    def get_item_representation(self):\n",
        "        \n",
        "        if self.use_metadata:\n",
        "            \n",
        "            data = (self.dataset\n",
        "                    .dataset[['item_id'] + self.dataset.metadata_id]\n",
        "                    .drop_duplicates())\n",
        "            \n",
        "            mapping = pd.get_dummies(data, columns=[*self.dataset.metadata_id]).values[:, 1:]\n",
        "            identity = np.identity(self.dataset.dataset['item_id'].max() + 1)\n",
        "            binary = np.hstack([identity, mapping])\n",
        "            \n",
        "            metadata_representation = np.vstack([self.item.weight.detach().numpy(), self.metadata.weight.detach().numpy()])\n",
        "            \n",
        "            return np.dot(binary, metadata_representation), binary, metadata_representation\n",
        "        \n",
        "        else:\n",
        "            return self.item.weight.cpu().detach().numpy()"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkRQhNzNPwnw"
      },
      "source": [
        "class NeuCF(CFTemplate):\n",
        "    \n",
        "    def __init__(self, dataset, n_factors, use_metadata=True):\n",
        "        super().__init__(dataset, n_factors)\n",
        "        \n",
        "        self.use_metadata = use_metadata\n",
        "        \n",
        "        if use_metadata:\n",
        "            self.n_metadata = self._get_n_metadata(dataset)\n",
        "            self.n_metadata_type = self._get_n_metadata_type(dataset)\n",
        "            self.metadata_gmf = ScaledEmbedding(self.n_metadata, n_factors)\n",
        "            self.metadata_mlp = ScaledEmbedding(self.n_metadata, n_factors)\n",
        "            \n",
        "        else:\n",
        "            self.n_metadata_type = 0\n",
        "    \n",
        "        self.user_gmf = ScaledEmbedding(self.n_users, self.n_factors)\n",
        "        self.user_mlp = ScaledEmbedding(self.n_users, self.n_factors)\n",
        "        \n",
        "        self.item_gmf = ScaledEmbedding(self.n_items, self.n_factors)\n",
        "        self.item_mlp = ScaledEmbedding(self.n_items, self.n_factors)\n",
        "        \n",
        "        self.linear_1 = torch.nn.Linear(n_factors*(2+self.n_metadata_type), self.n_factors*4)\n",
        "        self.linear_2 = torch.nn.Linear(self.n_factors*4, self.n_factors*2)\n",
        "        self.linear_3 = torch.nn.Linear(self.n_factors*2, self.n_factors)\n",
        "        self.linear_4 = torch.nn.Linear(self.n_factors*2, 1)\n",
        "        \n",
        "        self.weights = torch.nn.Parameter(torch.rand(2), requires_grad=True)\n",
        "        \n",
        "    def _get_n_metadata(self, dataset):\n",
        "        \n",
        "        n_metadata = 0\n",
        "        \n",
        "        for col in dataset.metadata_id:\n",
        "            n_metadata += dataset.dataset[col].max() + 1\n",
        "        \n",
        "        return n_metadata\n",
        "    \n",
        "    def _get_n_metadata_type(self, dataset):\n",
        "        \n",
        "        return len(dataset.metadata)\n",
        "    \n",
        "        \n",
        "    def gmf(self, dataset, batch_size=1):\n",
        "        \n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        \n",
        "        user = Variable(torch.LongTensor(dataset['user_id'].values))\n",
        "        item = Variable(torch.LongTensor(dataset['item_id'].values))\n",
        "        \n",
        "        if self.use_metadata:\n",
        "            metadata = Variable(torch.LongTensor(list(dataset['metadata'])))\n",
        "            metadata = self.metadata_gmf(metadata)\n",
        "            \n",
        "        user = self.user_gmf(user)\n",
        "        item = self.item_gmf(item)\n",
        "        \n",
        "        if self.use_metadata:\n",
        "            item = item.reshape(batch_size, 1, self.n_factors)        \n",
        "            item_metadata = torch.cat([item, metadata], axis=1)\n",
        "\n",
        "            ### sum of latent dimensions\n",
        "            item = item_metadata.sum(1)\n",
        "        \n",
        "        #net = (user * item).sum(1).view(-1,1) \n",
        "        net = (user * item)\n",
        "        \n",
        "        return net\n",
        "    \n",
        "    def mlp(self, dataset, batch_size=1):\n",
        "        \n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        user = Variable(torch.LongTensor(dataset['user_id'].values))\n",
        "        item = Variable(torch.LongTensor(dataset['item_id'].values))\n",
        "        \n",
        "        if self.use_metadata:\n",
        "            metadata = Variable(torch.LongTensor(list(dataset['metadata'])))\n",
        "            metadata = self.metadata_mlp(metadata).reshape(batch_size, self.n_factors*self.n_metadata_type)\n",
        "            \n",
        "        user = self.user_mlp(user)\n",
        "        item = self.item_mlp(item)\n",
        "        \n",
        "        if self.use_metadata:\n",
        "            cat = torch.cat([user, item, metadata], axis=1).reshape(batch_size, (2+self.n_metadata_type)*self.n_factors)\n",
        "        else:\n",
        "            cat = torch.cat([user, item], axis=1).reshape(batch_size, 2*self.n_factors)\n",
        "                \n",
        "        net = self.linear_1(cat)\n",
        "        net = torch.nn.functional.relu(net)\n",
        "        \n",
        "        net = self.linear_2(net)\n",
        "        net = torch.nn.functional.relu(net)\n",
        "        \n",
        "        net = self.linear_3(net)\n",
        "        \n",
        "        return net\n",
        "    \n",
        "    def net(self, dataset, batch_size=1):\n",
        "        \n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        user = Variable(torch.LongTensor(dataset['user_id'].values))\n",
        "        item = Variable(torch.LongTensor(dataset['item_id'].values))\n",
        "        \n",
        "        gmf = self.gmf(dataset, batch_size)\n",
        "        mlp = self.mlp(dataset, batch_size)\n",
        "        \n",
        "        net = torch.cat([gmf, mlp], axis=1)\n",
        "        \n",
        "        net = self.linear_4(net)\n",
        "#        net = torch.nn.functional.sigmoid(net)\n",
        "#         net = (self.weights * net).sum(1)\n",
        "                \n",
        "        return net"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jxRxBm2PyZy"
      },
      "source": [
        "class EASE():\n",
        "    \n",
        "    def __init__(self, dataset, split_train_test=True):\n",
        "        \n",
        "        self.dataset = dataset\n",
        "        self.item_dict = dataset.dataset.set_index('item_id')['product'].to_dict()\n",
        "        self.split_train_test = split_train_test\n",
        "        self._split_train_test()\n",
        "        \n",
        "    \n",
        "    def _split_train_test(self):\n",
        "        \n",
        "        if self.split_train_test:\n",
        "        \n",
        "            print('|== Splitting Train/Test ==|')\n",
        "            \n",
        "            data = self.dataset.dataset.iloc[np.random.permutation(len(self.dataset.dataset))]\n",
        "            self.train = data.iloc[:int(len(data) * 0.9)]\n",
        "            self.test = data.iloc[int(len(data) * 0.9):]\n",
        "            \n",
        "            print(f'Shape Train: {len(self.train)} \\nShape Test: {len(self.test)}')\n",
        "            \n",
        "        else:\n",
        "            \n",
        "            self.train = self.dataset.dataset\n",
        "            \n",
        "    \n",
        "    def fit(self, lambda_: float = 0.5, implicit=True):\n",
        "        \n",
        "        if implicit:\n",
        "            values = np.ones(self.train.shape[0])\n",
        "        else:\n",
        "            values = self.train.loc[:, 'action']\n",
        "        \n",
        "        users = self.train.loc[:, 'user_id']\n",
        "        items = self.train.loc[:, 'item_id']\n",
        "        \n",
        "        matrix = sp.csr_matrix((values, (users, items)))\n",
        "        self.matrix = matrix\n",
        "        \n",
        "        ### Weight Bij are\n",
        "        ### 0s if i=j (diagonal)\n",
        "        ### -Pij / Pjj otherwise\n",
        "        ### where P = Xt * X - lambda*I\n",
        "        \n",
        "        g = matrix.T.dot(matrix).toarray() \n",
        "        \n",
        "        diagonal = np.diag_indices(g.shape[0])\n",
        "        g[diagonal] += lambda_ ### => gives P\n",
        "        \n",
        "        p = np.linalg.inv(g)\n",
        "        \n",
        "        b = p / (-np.diag(p)) ### => gives Bij\n",
        "        b[diagonal] = 0       ### and sets diagonal to 0\n",
        "        \n",
        "        self.b = b\n",
        "        self.pred = matrix.dot(b)\n",
        "        \n",
        "    \n",
        "    def predict(self, user_id, k):\n",
        "        \n",
        "        user_prediction = self.pred[user_id, :]\n",
        "        item_ranked = np.argsort(-user_prediction)[:k]\n",
        "        \n",
        "        item_ranked = [self.item_dict[item] for item in item_ranked]\n",
        "        \n",
        "        return item_ranked\n",
        "    \n",
        "    \n",
        "    def get_similarity(self, k=10):\n",
        "        \n",
        "        similarity = {}\n",
        "        \n",
        "        for idx, row in enumerate(self.b):\n",
        "            \n",
        "            sorted_index = np.argsort(-row)[:k]\n",
        "            sorted_codes = [self.item_dict[item] for item in sorted_index]\n",
        "            similarity.update({self.item_dict[idx]: sorted_codes})\n",
        "            \n",
        "        return similarity"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Do1mhHKSP0Ee"
      },
      "source": [
        "### Main"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCM0v3pdQnPs"
      },
      "source": [
        "dataset = Dataset(brand='missoni')\n",
        "dataset.fit(metadata=['saleLine'])"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjAa6grZOlgg"
      },
      "source": [
        "config = {'model': ['lightfm', 'mlp'],\n",
        "          'metadata': [True, False],\n",
        "          'n_factors': [64, 128]}\n",
        "\n",
        "is_verbose=True\n",
        "epochs=20"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-j0RV-wWQYTR"
      },
      "source": [
        "import datetime"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fD9RUnU7OzkU",
        "outputId": "bacb2755-f5f1-4f81-d78c-cc8daf41b775"
      },
      "source": [
        "performance = None\n",
        "id = 0\n",
        "batch_size = int(dataset.dataset.shape[0] / 1000)\n",
        "print(f'Batch Size: {batch_size}') \n",
        "\n",
        "for model in config['model']:\n",
        "  for is_metadata in config['metadata']:\n",
        "    for factors in config['n_factors']:\n",
        "\n",
        "      print(f'Model: {model}, Metadata: {is_metadata}, Factors: {factors} \\n')\n",
        "      start = datetime.datetime.now()\n",
        "\n",
        "      if model == 'lightfm':\n",
        "\n",
        "        modello = LightFM(dataset=dataset,\n",
        "                          n_factors=factors,\n",
        "                          use_metadata=is_metadata,\n",
        "                          use_cuda=True)\n",
        "        \n",
        "      else: \n",
        "\n",
        "        modello = MLP(dataset=dataset,\n",
        "                          n_factors=factors,\n",
        "                          use_metadata=is_metadata,\n",
        "                          use_cuda=True)\n",
        "        \n",
        "      optimizer_model = torch.optim.Adam(modello.parameters(), \n",
        "                                    lr=1e-3,\n",
        "                                    weight_decay=1e-6)\n",
        "      \n",
        "      modello.fit(optimizer=optimizer_model, \n",
        "                  batch_size=batch_size, \n",
        "                  epochs=epochs, \n",
        "                  split_train_test=True,\n",
        "                  verbose=is_verbose)\n",
        "      \n",
        "      end = datetime.datetime.now()\n",
        "      runtime = (end - start).seconds\n",
        "\n",
        "      print(f'\\n Runtime: {runtime} \\n')\n",
        "\n",
        "      id += 1\n",
        "      \n",
        "      single = pd.DataFrame({'id': id,\n",
        "                            'model': model,\n",
        "                            'use_metadata': is_metadata,\n",
        "                            'factors': factors,\n",
        "                            'last_train_AUC': modello.history()['train_auc'][epochs-1],\n",
        "                            'last_test_AUC': modello.history()['test_auc'][epochs-1],\n",
        "                            'train_AUC': [modello.history()['train_auc']],\n",
        "                            'test_AUC': [modello.history()['test_auc']],\n",
        "                            'runtime': [runtime]})\n",
        "      \n",
        "      performance = pd.concat([performance, single])"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch Size: 726\n",
            "Model: lightfm, Metadata: True, Factors: 64 \n",
            "\n",
            "|== Splitting Train/Test ==|\n",
            "Shape Train: 653454 \n",
            "Shape Test: 72606\n",
            "Epoch: 1\n",
            "Epoch 1, \n",
            "== Loss: 0.4897219240665436 \n",
            "== Train AUC: 0.82095 \n",
            "== Test AUC: 0.8065\n",
            "Epoch: 2\n",
            "Epoch 2, \n",
            "== Loss: 0.2467082142829895 \n",
            "== Train AUC: 0.844 \n",
            "== Test AUC: 0.8291\n",
            "Epoch: 3\n",
            "Epoch 3, \n",
            "== Loss: 0.30169913172721863 \n",
            "== Train AUC: 0.85465 \n",
            "== Test AUC: 0.8421\n",
            "Epoch: 4\n",
            "Epoch 4, \n",
            "== Loss: 0.19369842112064362 \n",
            "== Train AUC: 0.8699 \n",
            "== Test AUC: 0.8508\n",
            "Epoch: 5\n",
            "Epoch 5, \n",
            "== Loss: 0.205636665225029 \n",
            "== Train AUC: 0.8733 \n",
            "== Test AUC: 0.8602\n",
            "Epoch: 6\n",
            "Epoch 6, \n",
            "== Loss: 0.151855006814003 \n",
            "== Train AUC: 0.8869 \n",
            "== Test AUC: 0.86915\n",
            "Epoch: 7\n",
            "Epoch 7, \n",
            "== Loss: 0.20444634556770325 \n",
            "== Train AUC: 0.8932 \n",
            "== Test AUC: 0.87515\n",
            "Epoch: 8\n",
            "Epoch 8, \n",
            "== Loss: 0.22585994005203247 \n",
            "== Train AUC: 0.90345 \n",
            "== Test AUC: 0.88135\n",
            "Epoch: 9\n",
            "Epoch 9, \n",
            "== Loss: 0.14001056551933289 \n",
            "== Train AUC: 0.90635 \n",
            "== Test AUC: 0.88395\n",
            "Epoch: 10\n",
            "Epoch 10, \n",
            "== Loss: 0.17071206867694855 \n",
            "== Train AUC: 0.91055 \n",
            "== Test AUC: 0.88305\n",
            "Epoch: 11\n",
            "Epoch 11, \n",
            "== Loss: 0.1867993026971817 \n",
            "== Train AUC: 0.9164 \n",
            "== Test AUC: 0.89145\n",
            "Epoch: 12\n",
            "Epoch 12, \n",
            "== Loss: 0.07964073121547699 \n",
            "== Train AUC: 0.9222 \n",
            "== Test AUC: 0.89935\n",
            "Epoch: 13\n",
            "Epoch 13, \n",
            "== Loss: 0.09339618682861328 \n",
            "== Train AUC: 0.9277 \n",
            "== Test AUC: 0.8986\n",
            "Epoch: 14\n",
            "Epoch 14, \n",
            "== Loss: 0.0319780670106411 \n",
            "== Train AUC: 0.92885 \n",
            "== Test AUC: 0.8973\n",
            "Epoch: 15\n",
            "Epoch 15, \n",
            "== Loss: 0.09121723473072052 \n",
            "== Train AUC: 0.93285 \n",
            "== Test AUC: 0.9027\n",
            "Epoch: 16\n",
            "Epoch 16, \n",
            "== Loss: 0.1321161687374115 \n",
            "== Train AUC: 0.9388 \n",
            "== Test AUC: 0.90095\n",
            "Epoch: 17\n",
            "Epoch 17, \n",
            "== Loss: 0.05088980123400688 \n",
            "== Train AUC: 0.93755 \n",
            "== Test AUC: 0.90135\n",
            "Epoch: 18\n",
            "Epoch 18, \n",
            "== Loss: 0.05853786692023277 \n",
            "== Train AUC: 0.93895 \n",
            "== Test AUC: 0.9006\n",
            "Epoch: 19\n",
            "Epoch 19, \n",
            "== Loss: 0.017894823104143143 \n",
            "== Train AUC: 0.945 \n",
            "== Test AUC: 0.9047\n",
            "Epoch: 20\n",
            "Epoch 20, \n",
            "== Loss: 0.057021621614694595 \n",
            "== Train AUC: 0.94785 \n",
            "== Test AUC: 0.90595\n",
            "\n",
            " Runtime: 119 \n",
            "\n",
            "Model: lightfm, Metadata: True, Factors: 128 \n",
            "\n",
            "|== Splitting Train/Test ==|\n",
            "Shape Train: 653454 \n",
            "Shape Test: 72606\n",
            "Epoch: 1\n",
            "Epoch 1, \n",
            "== Loss: 0.5377395749092102 \n",
            "== Train AUC: 0.8235 \n",
            "== Test AUC: 0.80845\n",
            "Epoch: 2\n",
            "Epoch 2, \n",
            "== Loss: 0.2882632315158844 \n",
            "== Train AUC: 0.8393 \n",
            "== Test AUC: 0.82815\n",
            "Epoch: 3\n",
            "Epoch 3, \n",
            "== Loss: 0.23188428580760956 \n",
            "== Train AUC: 0.86525 \n",
            "== Test AUC: 0.845\n",
            "Epoch: 4\n",
            "Epoch 4, \n",
            "== Loss: 0.19924679398536682 \n",
            "== Train AUC: 0.8796 \n",
            "== Test AUC: 0.862\n",
            "Epoch: 5\n",
            "Epoch 5, \n",
            "== Loss: 0.133272185921669 \n",
            "== Train AUC: 0.8949 \n",
            "== Test AUC: 0.8722\n",
            "Epoch: 6\n",
            "Epoch 6, \n",
            "== Loss: 0.27261510491371155 \n",
            "== Train AUC: 0.90135 \n",
            "== Test AUC: 0.87835\n",
            "Epoch: 7\n",
            "Epoch 7, \n",
            "== Loss: 0.1883954107761383 \n",
            "== Train AUC: 0.90675 \n",
            "== Test AUC: 0.88605\n",
            "Epoch: 8\n",
            "Epoch 8, \n",
            "== Loss: 0.16757117211818695 \n",
            "== Train AUC: 0.9156 \n",
            "== Test AUC: 0.8868\n",
            "Epoch: 9\n",
            "Epoch 9, \n",
            "== Loss: 0.10696572810411453 \n",
            "== Train AUC: 0.9193 \n",
            "== Test AUC: 0.8955\n",
            "Epoch: 10\n",
            "Epoch 10, \n",
            "== Loss: 0.1856699287891388 \n",
            "== Train AUC: 0.92925 \n",
            "== Test AUC: 0.8982\n",
            "Epoch: 11\n",
            "Epoch 11, \n",
            "== Loss: 0.09481693059206009 \n",
            "== Train AUC: 0.93005 \n",
            "== Test AUC: 0.9\n",
            "Epoch: 12\n",
            "Epoch 12, \n",
            "== Loss: 0.035813383758068085 \n",
            "== Train AUC: 0.93795 \n",
            "== Test AUC: 0.9058\n",
            "Epoch: 13\n",
            "Epoch 13, \n",
            "== Loss: 0.12048199772834778 \n",
            "== Train AUC: 0.9449 \n",
            "== Test AUC: 0.90155\n",
            "Epoch: 14\n",
            "Epoch 14, \n",
            "== Loss: 0.07139474153518677 \n",
            "== Train AUC: 0.94585 \n",
            "== Test AUC: 0.9027\n",
            "Epoch: 15\n",
            "Epoch 15, \n",
            "== Loss: 0.09611090272665024 \n",
            "== Train AUC: 0.9471 \n",
            "== Test AUC: 0.90945\n",
            "Epoch: 16\n",
            "Epoch 16, \n",
            "== Loss: 0.11924472451210022 \n",
            "== Train AUC: 0.95445 \n",
            "== Test AUC: 0.9072\n",
            "Epoch: 17\n",
            "Epoch 17, \n",
            "== Loss: 0.048362091183662415 \n",
            "== Train AUC: 0.95585 \n",
            "== Test AUC: 0.90825\n",
            "Epoch: 18\n",
            "Epoch 18, \n",
            "== Loss: 0.042874034494161606 \n",
            "== Train AUC: 0.9576 \n",
            "== Test AUC: 0.9091\n",
            "Epoch: 19\n",
            "Epoch 19, \n",
            "== Loss: 0.05372294783592224 \n",
            "== Train AUC: 0.9634 \n",
            "== Test AUC: 0.90955\n",
            "Epoch: 20\n",
            "Epoch 20, \n",
            "== Loss: 0.06771548092365265 \n",
            "== Train AUC: 0.96295 \n",
            "== Test AUC: 0.91035\n",
            "\n",
            " Runtime: 150 \n",
            "\n",
            "Model: lightfm, Metadata: False, Factors: 64 \n",
            "\n",
            "|== Splitting Train/Test ==|\n",
            "Shape Train: 653454 \n",
            "Shape Test: 72606\n",
            "Epoch: 1\n",
            "Epoch 1, \n",
            "== Loss: 0.516922652721405 \n",
            "== Train AUC: 0.8318 \n",
            "== Test AUC: 0.83285\n",
            "Epoch: 2\n",
            "Epoch 2, \n",
            "== Loss: 0.27881577610969543 \n",
            "== Train AUC: 0.8811 \n",
            "== Test AUC: 0.8638\n",
            "Epoch: 3\n",
            "Epoch 3, \n",
            "== Loss: 0.15142317116260529 \n",
            "== Train AUC: 0.8979 \n",
            "== Test AUC: 0.87405\n",
            "Epoch: 4\n",
            "Epoch 4, \n",
            "== Loss: 0.2233963906764984 \n",
            "== Train AUC: 0.90605 \n",
            "== Test AUC: 0.88985\n",
            "Epoch: 5\n",
            "Epoch 5, \n",
            "== Loss: 0.29845085740089417 \n",
            "== Train AUC: 0.9163 \n",
            "== Test AUC: 0.89425\n",
            "Epoch: 6\n",
            "Epoch 6, \n",
            "== Loss: 0.18901216983795166 \n",
            "== Train AUC: 0.9232 \n",
            "== Test AUC: 0.8997\n",
            "Epoch: 7\n",
            "Epoch 7, \n",
            "== Loss: 0.13910938799381256 \n",
            "== Train AUC: 0.92515 \n",
            "== Test AUC: 0.90095\n",
            "Epoch: 8\n",
            "Epoch 8, \n",
            "== Loss: 0.0392666757106781 \n",
            "== Train AUC: 0.9298 \n",
            "== Test AUC: 0.90405\n",
            "Epoch: 9\n",
            "Epoch 9, \n",
            "== Loss: 0.1858205646276474 \n",
            "== Train AUC: 0.93585 \n",
            "== Test AUC: 0.90755\n",
            "Epoch: 10\n",
            "Epoch 10, \n",
            "== Loss: 0.10409955680370331 \n",
            "== Train AUC: 0.94115 \n",
            "== Test AUC: 0.9081\n",
            "Epoch: 11\n",
            "Epoch 11, \n",
            "== Loss: 0.05927646532654762 \n",
            "== Train AUC: 0.9403 \n",
            "== Test AUC: 0.91135\n",
            "Epoch: 12\n",
            "Epoch 12, \n",
            "== Loss: 0.08467389643192291 \n",
            "== Train AUC: 0.9466 \n",
            "== Test AUC: 0.9116\n",
            "Epoch: 13\n",
            "Epoch 13, \n",
            "== Loss: 0.044656213372945786 \n",
            "== Train AUC: 0.95045 \n",
            "== Test AUC: 0.9097\n",
            "Epoch: 14\n",
            "Epoch 14, \n",
            "== Loss: 0.07080143690109253 \n",
            "== Train AUC: 0.95175 \n",
            "== Test AUC: 0.9135\n",
            "Epoch: 15\n",
            "Epoch 15, \n",
            "== Loss: 0.05334382504224777 \n",
            "== Train AUC: 0.9539 \n",
            "== Test AUC: 0.9115\n",
            "Epoch: 16\n",
            "Epoch 16, \n",
            "== Loss: 0.043953172862529755 \n",
            "== Train AUC: 0.95705 \n",
            "== Test AUC: 0.9142\n",
            "Epoch: 17\n",
            "Epoch 17, \n",
            "== Loss: 0.031233107671141624 \n",
            "== Train AUC: 0.95455 \n",
            "== Test AUC: 0.9119\n",
            "Epoch: 18\n",
            "Epoch 18, \n",
            "== Loss: 0.050581157207489014 \n",
            "== Train AUC: 0.9586 \n",
            "== Test AUC: 0.91355\n",
            "Epoch: 19\n",
            "Epoch 19, \n",
            "== Loss: 0.08743496984243393 \n",
            "== Train AUC: 0.96125 \n",
            "== Test AUC: 0.9114\n",
            "Epoch: 20\n",
            "Epoch 20, \n",
            "== Loss: 0.008455620147287846 \n",
            "== Train AUC: 0.96225 \n",
            "== Test AUC: 0.91195\n",
            "\n",
            " Runtime: 95 \n",
            "\n",
            "Model: lightfm, Metadata: False, Factors: 128 \n",
            "\n",
            "|== Splitting Train/Test ==|\n",
            "Shape Train: 653454 \n",
            "Shape Test: 72606\n",
            "Epoch: 1\n",
            "Epoch 1, \n",
            "== Loss: 0.5161147117614746 \n",
            "== Train AUC: 0.86535 \n",
            "== Test AUC: 0.8477\n",
            "Epoch: 2\n",
            "Epoch 2, \n",
            "== Loss: 0.3919789493083954 \n",
            "== Train AUC: 0.8979 \n",
            "== Test AUC: 0.87415\n",
            "Epoch: 3\n",
            "Epoch 3, \n",
            "== Loss: 0.27317529916763306 \n",
            "== Train AUC: 0.9109 \n",
            "== Test AUC: 0.88465\n",
            "Epoch: 4\n",
            "Epoch 4, \n",
            "== Loss: 0.23994767665863037 \n",
            "== Train AUC: 0.9215 \n",
            "== Test AUC: 0.89475\n",
            "Epoch: 5\n",
            "Epoch 5, \n",
            "== Loss: 0.14268885552883148 \n",
            "== Train AUC: 0.92905 \n",
            "== Test AUC: 0.8984\n",
            "Epoch: 6\n",
            "Epoch 6, \n",
            "== Loss: 0.0905623584985733 \n",
            "== Train AUC: 0.9352 \n",
            "== Test AUC: 0.90425\n",
            "Epoch: 7\n",
            "Epoch 7, \n",
            "== Loss: 0.18954779207706451 \n",
            "== Train AUC: 0.94015 \n",
            "== Test AUC: 0.90685\n",
            "Epoch: 8\n",
            "Epoch 8, \n",
            "== Loss: 0.12618431448936462 \n",
            "== Train AUC: 0.9434 \n",
            "== Test AUC: 0.90885\n",
            "Epoch: 9\n",
            "Epoch 9, \n",
            "== Loss: 0.06450238823890686 \n",
            "== Train AUC: 0.95295 \n",
            "== Test AUC: 0.9082\n",
            "Epoch: 10\n",
            "Epoch 10, \n",
            "== Loss: 0.09137221425771713 \n",
            "== Train AUC: 0.9535 \n",
            "== Test AUC: 0.9104\n",
            "Epoch: 11\n",
            "Epoch 11, \n",
            "== Loss: 0.05740082263946533 \n",
            "== Train AUC: 0.95635 \n",
            "== Test AUC: 0.9139\n",
            "Epoch: 12\n",
            "Epoch 12, \n",
            "== Loss: 0.08194088190793991 \n",
            "== Train AUC: 0.9601 \n",
            "== Test AUC: 0.9137\n",
            "Epoch: 13\n",
            "Epoch 13, \n",
            "== Loss: 0.06064498797059059 \n",
            "== Train AUC: 0.9637 \n",
            "== Test AUC: 0.9117\n",
            "Epoch: 14\n",
            "Epoch 14, \n",
            "== Loss: 0.03490626439452171 \n",
            "== Train AUC: 0.9644 \n",
            "== Test AUC: 0.9112\n",
            "Epoch: 15\n",
            "Epoch 15, \n",
            "== Loss: 0.03132600337266922 \n",
            "== Train AUC: 0.96845 \n",
            "== Test AUC: 0.91085\n",
            "Epoch: 16\n",
            "Epoch 16, \n",
            "== Loss: 0.004506782162934542 \n",
            "== Train AUC: 0.96685 \n",
            "== Test AUC: 0.9084\n",
            "Epoch: 17\n",
            "Epoch 17, \n",
            "== Loss: 0.015026147477328777 \n",
            "== Train AUC: 0.9719 \n",
            "== Test AUC: 0.90965\n",
            "Epoch: 18\n",
            "Epoch 18, \n",
            "== Loss: 0.037855759263038635 \n",
            "== Train AUC: 0.9722 \n",
            "== Test AUC: 0.9096\n",
            "Epoch: 19\n",
            "Epoch 19, \n",
            "== Loss: 0.07856211066246033 \n",
            "== Train AUC: 0.9719 \n",
            "== Test AUC: 0.91025\n",
            "Epoch: 20\n",
            "Epoch 20, \n",
            "== Loss: 0.03935002535581589 \n",
            "== Train AUC: 0.9748 \n",
            "== Test AUC: 0.9113\n",
            "\n",
            " Runtime: 128 \n",
            "\n",
            "Model: mlp, Metadata: True, Factors: 64 \n",
            "\n",
            "|== Splitting Train/Test ==|\n",
            "Shape Train: 653454 \n",
            "Shape Test: 72606\n",
            "Epoch: 1\n",
            "Epoch 1, \n",
            "== Loss: 0.6052988767623901 \n",
            "== Train AUC: 0.7886 \n",
            "== Test AUC: 0.786\n",
            "Epoch: 2\n",
            "Epoch 2, \n",
            "== Loss: 0.36894869804382324 \n",
            "== Train AUC: 0.82615 \n",
            "== Test AUC: 0.81425\n",
            "Epoch: 3\n",
            "Epoch 3, \n",
            "== Loss: 0.3739337921142578 \n",
            "== Train AUC: 0.8459 \n",
            "== Test AUC: 0.8345\n",
            "Epoch: 4\n",
            "Epoch 4, \n",
            "== Loss: 0.30694451928138733 \n",
            "== Train AUC: 0.85925 \n",
            "== Test AUC: 0.84735\n",
            "Epoch: 5\n",
            "Epoch 5, \n",
            "== Loss: 0.26505300402641296 \n",
            "== Train AUC: 0.865 \n",
            "== Test AUC: 0.849\n",
            "Epoch: 6\n",
            "Epoch 6, \n",
            "== Loss: 0.3068181872367859 \n",
            "== Train AUC: 0.87435 \n",
            "== Test AUC: 0.8551\n",
            "Epoch: 7\n",
            "Epoch 7, \n",
            "== Loss: 0.27268850803375244 \n",
            "== Train AUC: 0.87965 \n",
            "== Test AUC: 0.86145\n",
            "Epoch: 8\n",
            "Epoch 8, \n",
            "== Loss: 0.3480338454246521 \n",
            "== Train AUC: 0.88775 \n",
            "== Test AUC: 0.8648\n",
            "Epoch: 9\n",
            "Epoch 9, \n",
            "== Loss: 0.17187407612800598 \n",
            "== Train AUC: 0.89195 \n",
            "== Test AUC: 0.8742\n",
            "Epoch: 10\n",
            "Epoch 10, \n",
            "== Loss: 0.14981164038181305 \n",
            "== Train AUC: 0.8955 \n",
            "== Test AUC: 0.8765\n",
            "Epoch: 11\n",
            "Epoch 11, \n",
            "== Loss: 0.21318212151527405 \n",
            "== Train AUC: 0.89865 \n",
            "== Test AUC: 0.87755\n",
            "Epoch: 12\n",
            "Epoch 12, \n",
            "== Loss: 0.10887958109378815 \n",
            "== Train AUC: 0.9025 \n",
            "== Test AUC: 0.87865\n",
            "Epoch: 13\n",
            "Epoch 13, \n",
            "== Loss: 0.23187312483787537 \n",
            "== Train AUC: 0.90465 \n",
            "== Test AUC: 0.8777\n",
            "Epoch: 14\n",
            "Epoch 14, \n",
            "== Loss: 0.15570633113384247 \n",
            "== Train AUC: 0.9043 \n",
            "== Test AUC: 0.88465\n",
            "Epoch: 15\n",
            "Epoch 15, \n",
            "== Loss: 0.12938088178634644 \n",
            "== Train AUC: 0.90655 \n",
            "== Test AUC: 0.88625\n",
            "Epoch: 16\n",
            "Epoch 16, \n",
            "== Loss: 0.01939922757446766 \n",
            "== Train AUC: 0.90935 \n",
            "== Test AUC: 0.8872\n",
            "Epoch: 17\n",
            "Epoch 17, \n",
            "== Loss: 0.20926377177238464 \n",
            "== Train AUC: 0.9132 \n",
            "== Test AUC: 0.8851\n",
            "Epoch: 18\n",
            "Epoch 18, \n",
            "== Loss: 0.1510937362909317 \n",
            "== Train AUC: 0.91515 \n",
            "== Test AUC: 0.88685\n",
            "Epoch: 19\n",
            "Epoch 19, \n",
            "== Loss: 0.08917595446109772 \n",
            "== Train AUC: 0.91655 \n",
            "== Test AUC: 0.88715\n",
            "Epoch: 20\n",
            "Epoch 20, \n",
            "== Loss: 0.15733028948307037 \n",
            "== Train AUC: 0.9155 \n",
            "== Test AUC: 0.8904\n",
            "\n",
            " Runtime: 126 \n",
            "\n",
            "Model: mlp, Metadata: True, Factors: 128 \n",
            "\n",
            "|== Splitting Train/Test ==|\n",
            "Shape Train: 653454 \n",
            "Shape Test: 72606\n",
            "Epoch: 1\n",
            "Epoch 1, \n",
            "== Loss: 0.6645969748497009 \n",
            "== Train AUC: 0.80285 \n",
            "== Test AUC: 0.78675\n",
            "Epoch: 2\n",
            "Epoch 2, \n",
            "== Loss: 0.4092752933502197 \n",
            "== Train AUC: 0.8467 \n",
            "== Test AUC: 0.8362\n",
            "Epoch: 3\n",
            "Epoch 3, \n",
            "== Loss: 0.35461583733558655 \n",
            "== Train AUC: 0.8609 \n",
            "== Test AUC: 0.84805\n",
            "Epoch: 4\n",
            "Epoch 4, \n",
            "== Loss: 0.3457005023956299 \n",
            "== Train AUC: 0.8746 \n",
            "== Test AUC: 0.85635\n",
            "Epoch: 5\n",
            "Epoch 5, \n",
            "== Loss: 0.1714625060558319 \n",
            "== Train AUC: 0.88295 \n",
            "== Test AUC: 0.85995\n",
            "Epoch: 6\n",
            "Epoch 6, \n",
            "== Loss: 0.3251591920852661 \n",
            "== Train AUC: 0.88835 \n",
            "== Test AUC: 0.86465\n",
            "Epoch: 7\n",
            "Epoch 7, \n",
            "== Loss: 0.1490851193666458 \n",
            "== Train AUC: 0.8894 \n",
            "== Test AUC: 0.8726\n",
            "Epoch: 8\n",
            "Epoch 8, \n",
            "== Loss: 0.23545102775096893 \n",
            "== Train AUC: 0.90075 \n",
            "== Test AUC: 0.87805\n",
            "Epoch: 9\n",
            "Epoch 9, \n",
            "== Loss: 0.5089142322540283 \n",
            "== Train AUC: 0.90775 \n",
            "== Test AUC: 0.87865\n",
            "Epoch: 10\n",
            "Epoch 10, \n",
            "== Loss: 0.1320725977420807 \n",
            "== Train AUC: 0.91235 \n",
            "== Test AUC: 0.881\n",
            "Epoch: 11\n",
            "Epoch 11, \n",
            "== Loss: 0.17324073612689972 \n",
            "== Train AUC: 0.91165 \n",
            "== Test AUC: 0.8869\n",
            "Epoch: 12\n",
            "Epoch 12, \n",
            "== Loss: 0.11456570029258728 \n",
            "== Train AUC: 0.91305 \n",
            "== Test AUC: 0.8849\n",
            "Epoch: 13\n",
            "Epoch 13, \n",
            "== Loss: 0.23760898411273956 \n",
            "== Train AUC: 0.91575 \n",
            "== Test AUC: 0.88655\n",
            "Epoch: 14\n",
            "Epoch 14, \n",
            "== Loss: 0.06772701442241669 \n",
            "== Train AUC: 0.9166 \n",
            "== Test AUC: 0.8882\n",
            "Epoch: 15\n",
            "Epoch 15, \n",
            "== Loss: 0.1838667094707489 \n",
            "== Train AUC: 0.9201 \n",
            "== Test AUC: 0.88695\n",
            "Epoch: 16\n",
            "Epoch 16, \n",
            "== Loss: 0.12145614624023438 \n",
            "== Train AUC: 0.91535 \n",
            "== Test AUC: 0.88695\n",
            "Epoch: 17\n",
            "Epoch 17, \n",
            "== Loss: 0.08554358035326004 \n",
            "== Train AUC: 0.91945 \n",
            "== Test AUC: 0.88795\n",
            "Epoch: 18\n",
            "Epoch 18, \n",
            "== Loss: 0.06185608357191086 \n",
            "== Train AUC: 0.9202 \n",
            "== Test AUC: 0.8913\n",
            "Epoch: 19\n",
            "Epoch 19, \n",
            "== Loss: 0.15517832338809967 \n",
            "== Train AUC: 0.92515 \n",
            "== Test AUC: 0.8916\n",
            "Epoch: 20\n",
            "Epoch 20, \n",
            "== Loss: 0.08186936378479004 \n",
            "== Train AUC: 0.9237 \n",
            "== Test AUC: 0.89345\n",
            "\n",
            " Runtime: 158 \n",
            "\n",
            "Model: mlp, Metadata: False, Factors: 64 \n",
            "\n",
            "|== Splitting Train/Test ==|\n",
            "Shape Train: 653454 \n",
            "Shape Test: 72606\n",
            "Epoch: 1\n",
            "Epoch 1, \n",
            "== Loss: 0.4588795006275177 \n",
            "== Train AUC: 0.7298 \n",
            "== Test AUC: 0.73015\n",
            "Epoch: 2\n",
            "Epoch 2, \n",
            "== Loss: 0.5419259071350098 \n",
            "== Train AUC: 0.7413 \n",
            "== Test AUC: 0.7392\n",
            "Epoch: 3\n",
            "Epoch 3, \n",
            "== Loss: 0.3983444273471832 \n",
            "== Train AUC: 0.81535 \n",
            "== Test AUC: 0.80485\n",
            "Epoch: 4\n",
            "Epoch 4, \n",
            "== Loss: 0.37153083086013794 \n",
            "== Train AUC: 0.84165 \n",
            "== Test AUC: 0.829\n",
            "Epoch: 5\n",
            "Epoch 5, \n",
            "== Loss: 0.1712561547756195 \n",
            "== Train AUC: 0.8561 \n",
            "== Test AUC: 0.83455\n",
            "Epoch: 6\n",
            "Epoch 6, \n",
            "== Loss: 0.12693412601947784 \n",
            "== Train AUC: 0.867 \n",
            "== Test AUC: 0.84715\n",
            "Epoch: 7\n",
            "Epoch 7, \n",
            "== Loss: 0.27237966656684875 \n",
            "== Train AUC: 0.87205 \n",
            "== Test AUC: 0.85315\n",
            "Epoch: 8\n",
            "Epoch 8, \n",
            "== Loss: 0.1520010232925415 \n",
            "== Train AUC: 0.88195 \n",
            "== Test AUC: 0.85935\n",
            "Epoch: 9\n",
            "Epoch 9, \n",
            "== Loss: 0.09478897601366043 \n",
            "== Train AUC: 0.8865 \n",
            "== Test AUC: 0.85835\n",
            "Epoch: 10\n",
            "Epoch 10, \n",
            "== Loss: 0.19182834029197693 \n",
            "== Train AUC: 0.89175 \n",
            "== Test AUC: 0.8674\n",
            "Epoch: 11\n",
            "Epoch 11, \n",
            "== Loss: 0.21262037754058838 \n",
            "== Train AUC: 0.8938 \n",
            "== Test AUC: 0.86765\n",
            "Epoch: 12\n",
            "Epoch 12, \n",
            "== Loss: 0.10797396302223206 \n",
            "== Train AUC: 0.89665 \n",
            "== Test AUC: 0.8674\n",
            "Epoch: 13\n",
            "Epoch 13, \n",
            "== Loss: 0.04160679876804352 \n",
            "== Train AUC: 0.90345 \n",
            "== Test AUC: 0.8752\n",
            "Epoch: 14\n",
            "Epoch 14, \n",
            "== Loss: 0.1982964128255844 \n",
            "== Train AUC: 0.9055 \n",
            "== Test AUC: 0.87665\n",
            "Epoch: 15\n",
            "Epoch 15, \n",
            "== Loss: 0.050088733434677124 \n",
            "== Train AUC: 0.9094 \n",
            "== Test AUC: 0.87525\n",
            "Epoch: 16\n",
            "Epoch 16, \n",
            "== Loss: 0.20295991003513336 \n",
            "== Train AUC: 0.9104 \n",
            "== Test AUC: 0.87815\n",
            "Epoch: 17\n",
            "Epoch 17, \n",
            "== Loss: 0.12326531857252121 \n",
            "== Train AUC: 0.9092 \n",
            "== Test AUC: 0.87855\n",
            "Epoch: 18\n",
            "Epoch 18, \n",
            "== Loss: 0.1454678624868393 \n",
            "== Train AUC: 0.90905 \n",
            "== Test AUC: 0.88035\n",
            "Epoch: 19\n",
            "Epoch 19, \n",
            "== Loss: 0.20143666863441467 \n",
            "== Train AUC: 0.91185 \n",
            "== Test AUC: 0.8843\n",
            "Epoch: 20\n",
            "Epoch 20, \n",
            "== Loss: 0.1305347979068756 \n",
            "== Train AUC: 0.91605 \n",
            "== Test AUC: 0.88145\n",
            "\n",
            " Runtime: 107 \n",
            "\n",
            "Model: mlp, Metadata: False, Factors: 128 \n",
            "\n",
            "|== Splitting Train/Test ==|\n",
            "Shape Train: 653454 \n",
            "Shape Test: 72606\n",
            "Epoch: 1\n",
            "Epoch 1, \n",
            "== Loss: 0.521456778049469 \n",
            "== Train AUC: 0.7295 \n",
            "== Test AUC: 0.7238\n",
            "Epoch: 2\n",
            "Epoch 2, \n",
            "== Loss: 0.5724787712097168 \n",
            "== Train AUC: 0.7889 \n",
            "== Test AUC: 0.783\n",
            "Epoch: 3\n",
            "Epoch 3, \n",
            "== Loss: 0.3297916054725647 \n",
            "== Train AUC: 0.8344 \n",
            "== Test AUC: 0.82135\n",
            "Epoch: 4\n",
            "Epoch 4, \n",
            "== Loss: 0.20267412066459656 \n",
            "== Train AUC: 0.8517 \n",
            "== Test AUC: 0.8374\n",
            "Epoch: 5\n",
            "Epoch 5, \n",
            "== Loss: 0.26696184277534485 \n",
            "== Train AUC: 0.86695 \n",
            "== Test AUC: 0.8461\n",
            "Epoch: 6\n",
            "Epoch 6, \n",
            "== Loss: 0.10344210267066956 \n",
            "== Train AUC: 0.8817 \n",
            "== Test AUC: 0.86285\n",
            "Epoch: 7\n",
            "Epoch 7, \n",
            "== Loss: 0.1724516898393631 \n",
            "== Train AUC: 0.8854 \n",
            "== Test AUC: 0.85935\n",
            "Epoch: 8\n",
            "Epoch 8, \n",
            "== Loss: 0.1979038119316101 \n",
            "== Train AUC: 0.89305 \n",
            "== Test AUC: 0.8695\n",
            "Epoch: 9\n",
            "Epoch 9, \n",
            "== Loss: 0.10974793881177902 \n",
            "== Train AUC: 0.8977 \n",
            "== Test AUC: 0.87085\n",
            "Epoch: 10\n",
            "Epoch 10, \n",
            "== Loss: 0.12611062824726105 \n",
            "== Train AUC: 0.90135 \n",
            "== Test AUC: 0.8778\n",
            "Epoch: 11\n",
            "Epoch 11, \n",
            "== Loss: 0.08828133344650269 \n",
            "== Train AUC: 0.9046 \n",
            "== Test AUC: 0.8723\n",
            "Epoch: 12\n",
            "Epoch 12, \n",
            "== Loss: 0.09407716244459152 \n",
            "== Train AUC: 0.90825 \n",
            "== Test AUC: 0.87965\n",
            "Epoch: 13\n",
            "Epoch 13, \n",
            "== Loss: 0.18062898516654968 \n",
            "== Train AUC: 0.9094 \n",
            "== Test AUC: 0.88055\n",
            "Epoch: 14\n",
            "Epoch 14, \n",
            "== Loss: 0.08808637410402298 \n",
            "== Train AUC: 0.9133 \n",
            "== Test AUC: 0.88195\n",
            "Epoch: 15\n",
            "Epoch 15, \n",
            "== Loss: 0.13128629326820374 \n",
            "== Train AUC: 0.91445 \n",
            "== Test AUC: 0.87965\n",
            "Epoch: 16\n",
            "Epoch 16, \n",
            "== Loss: 0.07304023206233978 \n",
            "== Train AUC: 0.91695 \n",
            "== Test AUC: 0.8801\n",
            "Epoch: 17\n",
            "Epoch 17, \n",
            "== Loss: 0.17657040059566498 \n",
            "== Train AUC: 0.91825 \n",
            "== Test AUC: 0.88365\n",
            "Epoch: 18\n",
            "Epoch 18, \n",
            "== Loss: 0.16844014823436737 \n",
            "== Train AUC: 0.9195 \n",
            "== Test AUC: 0.8893\n",
            "Epoch: 19\n",
            "Epoch 19, \n",
            "== Loss: 0.11400455981492996 \n",
            "== Train AUC: 0.9224 \n",
            "== Test AUC: 0.88475\n",
            "Epoch: 20\n",
            "Epoch 20, \n",
            "== Loss: 0.037689678370952606 \n",
            "== Train AUC: 0.92195 \n",
            "== Test AUC: 0.88475\n",
            "\n",
            " Runtime: 142 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2cJzd94eEQ0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "a49a5e90-11ec-49e5-95f3-6adf2aa80853"
      },
      "source": [
        "performance.sort_values('last_test_AUC', ascending=False)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>model</th>\n",
              "      <th>use_metadata</th>\n",
              "      <th>factors</th>\n",
              "      <th>last_train_AUC</th>\n",
              "      <th>last_test_AUC</th>\n",
              "      <th>train_AUC</th>\n",
              "      <th>test_AUC</th>\n",
              "      <th>runtime</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>lightfm</td>\n",
              "      <td>False</td>\n",
              "      <td>64</td>\n",
              "      <td>0.96225</td>\n",
              "      <td>0.91195</td>\n",
              "      <td>[0.8318, 0.8811, 0.8979, 0.90605, 0.9163, 0.92...</td>\n",
              "      <td>[0.83285, 0.8638, 0.87405, 0.88985, 0.89425, 0...</td>\n",
              "      <td>95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>lightfm</td>\n",
              "      <td>False</td>\n",
              "      <td>128</td>\n",
              "      <td>0.97480</td>\n",
              "      <td>0.91130</td>\n",
              "      <td>[0.86535, 0.8979, 0.9109, 0.9215, 0.92905, 0.9...</td>\n",
              "      <td>[0.8477, 0.87415, 0.88465, 0.89475, 0.8984, 0....</td>\n",
              "      <td>128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>lightfm</td>\n",
              "      <td>True</td>\n",
              "      <td>128</td>\n",
              "      <td>0.96295</td>\n",
              "      <td>0.91035</td>\n",
              "      <td>[0.8235, 0.8393, 0.86525, 0.8796, 0.8949, 0.90...</td>\n",
              "      <td>[0.80845, 0.82815, 0.845, 0.862, 0.8722, 0.878...</td>\n",
              "      <td>150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>lightfm</td>\n",
              "      <td>True</td>\n",
              "      <td>64</td>\n",
              "      <td>0.94785</td>\n",
              "      <td>0.90595</td>\n",
              "      <td>[0.82095, 0.844, 0.85465, 0.8699, 0.8733, 0.88...</td>\n",
              "      <td>[0.8065, 0.8291, 0.8421, 0.8508, 0.8602, 0.869...</td>\n",
              "      <td>119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>mlp</td>\n",
              "      <td>True</td>\n",
              "      <td>128</td>\n",
              "      <td>0.92370</td>\n",
              "      <td>0.89345</td>\n",
              "      <td>[0.80285, 0.8467, 0.8609, 0.8746, 0.88295, 0.8...</td>\n",
              "      <td>[0.78675, 0.8362, 0.84805, 0.85635, 0.85995, 0...</td>\n",
              "      <td>158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>mlp</td>\n",
              "      <td>True</td>\n",
              "      <td>64</td>\n",
              "      <td>0.91550</td>\n",
              "      <td>0.89040</td>\n",
              "      <td>[0.7886, 0.82615, 0.8459, 0.85925, 0.865, 0.87...</td>\n",
              "      <td>[0.786, 0.81425, 0.8345, 0.84735, 0.849, 0.855...</td>\n",
              "      <td>126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8</td>\n",
              "      <td>mlp</td>\n",
              "      <td>False</td>\n",
              "      <td>128</td>\n",
              "      <td>0.92195</td>\n",
              "      <td>0.88475</td>\n",
              "      <td>[0.7295, 0.7889, 0.8344, 0.8517, 0.86695, 0.88...</td>\n",
              "      <td>[0.7238, 0.783, 0.82135, 0.8374, 0.8461, 0.862...</td>\n",
              "      <td>142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7</td>\n",
              "      <td>mlp</td>\n",
              "      <td>False</td>\n",
              "      <td>64</td>\n",
              "      <td>0.91605</td>\n",
              "      <td>0.88145</td>\n",
              "      <td>[0.7298, 0.7413, 0.81535, 0.84165, 0.8561, 0.8...</td>\n",
              "      <td>[0.73015, 0.7392, 0.80485, 0.829, 0.83455, 0.8...</td>\n",
              "      <td>107</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id    model  ...                                           test_AUC  runtime\n",
              "0   3  lightfm  ...  [0.83285, 0.8638, 0.87405, 0.88985, 0.89425, 0...       95\n",
              "0   4  lightfm  ...  [0.8477, 0.87415, 0.88465, 0.89475, 0.8984, 0....      128\n",
              "0   2  lightfm  ...  [0.80845, 0.82815, 0.845, 0.862, 0.8722, 0.878...      150\n",
              "0   1  lightfm  ...  [0.8065, 0.8291, 0.8421, 0.8508, 0.8602, 0.869...      119\n",
              "0   6      mlp  ...  [0.78675, 0.8362, 0.84805, 0.85635, 0.85995, 0...      158\n",
              "0   5      mlp  ...  [0.786, 0.81425, 0.8345, 0.84735, 0.849, 0.855...      126\n",
              "0   8      mlp  ...  [0.7238, 0.783, 0.82135, 0.8374, 0.8461, 0.862...      142\n",
              "0   7      mlp  ...  [0.73015, 0.7392, 0.80485, 0.829, 0.83455, 0.8...      107\n",
              "\n",
              "[8 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QqgTJPVQyyl"
      },
      "source": [
        "if is_verbose:\n",
        "  import matplotlib.pyplot as plt\n",
        "  plt.plot(range(epochs), modello.history()['train_auc'], label='train_meta_auc')\n",
        "  plt.plot(range(epochs), modello.history()['test_auc'], label='test_meta_auc')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}